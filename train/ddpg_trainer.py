# training/ddpg_trainer.py
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import copy
import os
import time
import matplotlib.pyplot as plt
from config import config, Config
from training.utils import *
from utils.metrics import TrainingMetrics, EpisodeMetrics
from visualization.portfolio_visualizer import PortfolioVisualizer

class EnhancedTrainingManager:
    """å¢å¼ºç‰ˆè®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, actor):
        self.actor = actor
        self.concentration_history = []
        self.performance_history = []

    def step(self, episode, portfolio_return=None):
        """æ¯æ­¥è°ƒç”¨"""
        # è®°å½•è¡¨ç°
        if portfolio_return is not None:
            self.performance_history.append(portfolio_return)

        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if episode % 100 == 0:
            self._check_and_adjust(episode)

    def _check_and_adjust(self, episode):
        """æ£€æŸ¥å¹¶è°ƒæ•´è®­ç»ƒç­–ç•¥"""
        if len(self.performance_history) < 50:
            return

        recent_perf = self.performance_history[-50:]
        perf_std = np.std(recent_perf)

        # å¦‚æœè¡¨ç°æ³¢åŠ¨å¾ˆå°ï¼Œå¯èƒ½é™·å…¥äº†å±€éƒ¨æœ€ä¼˜
        if perf_std < 0.001:
            print(f"Episode {episode}: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ—©æ”¶æ•›ï¼Œå¢å¼ºæ¢ç´¢")
            self.actor.set_exploration_mode(high_exploration=True)
        else:
            self.actor.set_exploration_mode(high_exploration=False)




def train_ddpg_enhanced(env, actor, critic, replay_buffer, num_episodes=100, max_steps=200,
                        fund_names=None, visualization_freq=20, save_dir="./training_results",
                        config=None, resume_from_checkpoint=None):
    """
    å¢å¼ºç‰ˆDDPGè®­ç»ƒå‡½æ•° - æ”¯æŒæ–°ç‰ˆImprovedActorçš„æ‰€æœ‰åŠŸèƒ½

    ä¸»è¦æ”¹è¿›ï¼š
    1. é›†æˆEnhancedTrainingManagerè¿›è¡Œæ™ºèƒ½è®­ç»ƒç®¡ç†
    2. æ”¯æŒåŠ¨æ€æ¢ç´¢ç­–ç•¥è°ƒæ•´
    3. æ€§èƒ½è®°å¿†æ›´æ–°å’Œè½¯é‡ç½®æœºåˆ¶
    4. æ›´è¯¦ç»†çš„è¯Šæ–­å’Œç›‘æ§
    5. è‡ªé€‚åº”è®­ç»ƒå‚æ•°è°ƒæ•´
    6. æ”¯æŒæ£€æŸ¥ç‚¹æ–­ç‚¹ç»­è®­

    Args:
        resume_from_checkpoint: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»è¯¥ç‚¹ç»§ç»­è®­ç»ƒ
    """

    # ä½¿ç”¨é…ç½®
    if config is None:
        config = Config()

    # éªŒè¯ç¯å¢ƒä¸é…ç½®çš„å…¼å®¹æ€§
    _validate_environment_compatibility(env, config)

    # åˆ›å»ºç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(f"{save_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{save_dir}/diagnostics", exist_ok=True)

    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ç½‘ç»œåˆå§‹åŒ–
    actor = actor.to(device)
    critic = critic.to(device)

    # ç›®æ ‡ç½‘ç»œ
    actor_target = copy.deepcopy(actor).to(device)
    critic_target = copy.deepcopy(critic).to(device)

    # ä¼˜åŒ–å™¨
    actor_optimizer = optim.Adam(actor.parameters(), lr=config.lr_actor)
    critic_optimizer = optim.Adam(critic.parameters(), lr=config.lr_critic)

    # ===== æ£€æŸ¥ç‚¹åŠ è½½é€»è¾‘ =====
    start_episode = 0
    loaded_training_metrics = None

    if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from_checkpoint}")
        start_episode, loaded_training_metrics = load_checkpoint(
            resume_from_checkpoint, actor, critic, actor_optimizer, critic_optimizer
        )
        print(f"âœ… æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œä»ç¬¬ {start_episode + 1} å›åˆç»§ç»­è®­ç»ƒ")
    else:
        if resume_from_checkpoint:
            print(f"âš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {resume_from_checkpoint}")
            print("ğŸ†• å¼€å§‹æ–°çš„è®­ç»ƒ")

    # ===== æ–°å¢ï¼šå¢å¼ºè®­ç»ƒç®¡ç†å™¨ =====
    training_manager = EnhancedTrainingManager(actor)

    # ===== æ¢ç´¢å‚æ•°è®¾ç½® =====
    exploration_params = {
        'start_rate': 0.5,
        'end_rate': 0.05,
        'decay_rate': 0.995,
        'diversity_reward_weight': 0.1,
        'concentration_threshold': 0.25,
        'force_diversify_threshold': 0.4,  # æ–°å¢ï¼šå¼ºåˆ¶å¤šæ ·åŒ–é˜ˆå€¼
        'performance_stagnation_threshold': 0.001  # æ–°å¢ï¼šæ€§èƒ½åœæ»é˜ˆå€¼
    }

    current_exploration_rate = exploration_params['start_rate']

    # å¯è§†åŒ–å™¨å’Œæ•°æ®å­˜å‚¨
    visualizer = PortfolioVisualizer()

    # ä½¿ç”¨åŠ è½½çš„è®­ç»ƒæŒ‡æ ‡æˆ–åˆ›å»ºæ–°çš„
    if loaded_training_metrics is not None:
        training_metrics = loaded_training_metrics
        print(f"ğŸ“Š å·²åŠ è½½ {len(training_metrics.episodes)} ä¸ªå†å²å›åˆçš„è®­ç»ƒæ•°æ®")
    else:
        training_metrics = TrainingMetrics()

    best_reward = -float('inf')

    # å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œè®¡ç®—å†å²æœ€ä½³å¥–åŠ±
    if loaded_training_metrics is not None and len(loaded_training_metrics.episodes) > 0:
        historical_rewards = [ep.total_reward for ep in loaded_training_metrics.episodes]
        best_reward = max(historical_rewards)
        print(f"ğŸ“ˆ å†å²æœ€ä½³å¥–åŠ±: {best_reward:.4f}")

    # ===== æ–°å¢ï¼šå¢å¼ºç›‘æ§å˜é‡ =====
    recent_performance_window = []
    concentration_violations = 0
    forced_diversification_count = 0
    last_step_return = None  # ç”¨äºä¼ é€’ç»™actorçš„portfolio_return

    # ä»æ£€æŸ¥ç‚¹æ¢å¤æ—¶ï¼Œå¡«å……æ€§èƒ½çª—å£
    if loaded_training_metrics is not None and len(loaded_training_metrics.episodes) > 0:
        # ä½¿ç”¨æœ€è¿‘çš„å›åˆæ•°æ®å¡«å……æ€§èƒ½çª—å£
        recent_episodes = loaded_training_metrics.episodes[-50:] if len(
            loaded_training_metrics.episodes) >= 50 else loaded_training_metrics.episodes
        recent_performance_window = [ep.total_reward for ep in recent_episodes]
        print(f"ğŸ“Š å·²æ¢å¤ {len(recent_performance_window)} ä¸ªå›åˆçš„æ€§èƒ½å†å²")

    print("å¼€å§‹è®­ç»ƒå¢å¼ºç‰ˆDDPG...")
    print(f"èµ„äº§æ•°é‡: {config.n_assets}")
    print(f"å› å­æ•°é‡: {config.n_factors}")
    print(f"æ—¶é—´çª—å£: {config.lookback_window}")
    print(f"Actorç‰¹æ€§: å¤šå¤´å†³ç­– + åŠ¨æ€åç½® + æ€§èƒ½è®°å¿†")
    print(f"è®­ç»ƒç®¡ç†: æ™ºèƒ½æ¢ç´¢è°ƒæ•´ + è½¯é‡ç½®æœºåˆ¶")
    print(f"è®­ç»ƒèŒƒå›´: Episode {start_episode + 1} -> {num_episodes}")

    for episode in range(start_episode, num_episodes):
        state = env.reset()
        episode_metrics = EpisodeMetrics()

        # éªŒè¯çŠ¶æ€ç»´åº¦
        _validate_state_dimensions(state, config)

        # ===== æ–°å¢ï¼šæ¯å›åˆå¼€å§‹æ—¶çš„ç­–ç•¥è°ƒæ•´ =====
        training_manager.step(episode)

        # éšæœºé€‰æ‹©æ¢ç´¢ç­–ç•¥ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        exploration_strategy = np.random.choice(
            ['gaussian', 'diversified', 'adaptive', 'default'],
            p=[0.3, 0.3, 0.2, 0.2]
        )

        episode_step_returns = []  # è®°å½•æœ¬å›åˆæ¯æ­¥çš„æ”¶ç›Š

        for step in range(max_steps):
            # ===== æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨æ–°ç‰ˆActor =====
            with torch.no_grad():
                # ä¼ å…¥ä¸Šä¸€æ­¥çš„æ”¶ç›Šæ›´æ–°æ€§èƒ½è®°å¿†
                action = actor(
                    state.unsqueeze(0),
                    add_noise=True,
                    training_mode=True,
                    portfolio_return=last_step_return
                ).squeeze(0)

            # éªŒè¯åŠ¨ä½œç»´åº¦
            assert len(action) == config.n_assets, f"åŠ¨ä½œç»´åº¦é”™è¯¯: æœŸæœ›{config.n_assets}, å®é™…{len(action)}"

            # ===== æ–°å¢ï¼šå®æ—¶ç›‘æ§å’Œè¯Šæ–­ =====
            if step % 20 == 0:  # æ¯20æ­¥ç›‘æ§ä¸€æ¬¡
                stats = actor.get_enhanced_stats(action.unsqueeze(0))

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶å¤šæ ·åŒ–
                if stats['hhi'] > exploration_params['force_diversify_threshold']:
                    print(f"ğŸš¨ Episode {episode + 1}, Step {step}: HHI={stats['hhi']:.4f} è¶…è¿‡é˜ˆå€¼ï¼Œè§¦å‘å¼ºåˆ¶å¤šæ ·åŒ–")
                    actor.force_diversify()
                    forced_diversification_count += 1

                    # é‡æ–°ç”Ÿæˆaction
                    action = actor(
                        state.unsqueeze(0),
                        add_noise=True,
                        training_mode=True,
                        portfolio_return=last_step_return
                    ).squeeze(0)

                if step % 100 == 0:  # è¯¦ç»†è¯Šæ–­
                    print(f"Episode {episode + 1}, Step {step} - å¢å¼ºè¯Šæ–­:")
                    print(f"  HHI: {stats['hhi']:.4f}, æœ‰æ•ˆèµ„äº§: {stats['effective_assets']}")
                    print(f"  æœ€å¤§æƒé‡: {stats['max_weight']:.4f}, æ¸©åº¦: {stats['temperature']:.4f}")
                    print(f"  æœªå……åˆ†ä½¿ç”¨èµ„äº§: {stats['unused_assets']}")

            # è®°å½•æƒé‡å’ŒæŒ‡æ ‡
            episode_metrics.add_step(action.detach().cpu().numpy(), step)

            # æ‰§è¡Œç¯å¢ƒæ­¥
            next_state, reward, done, info = env.step(action)

            # ===== æ–°å¢ï¼šè®¡ç®—æ­¥æ”¶ç›Šç‡ç”¨äºä¸‹ä¸€æ­¥ =====
            current_portfolio_value = info.get("portfolio_value", 0)
            if step > 0:
                # ä½¿ç”¨ values_history è·å–å‰ä¸€æ­¥çš„ä»·å€¼
                if len(episode_metrics.values_history) > 1:
                    previous_value = episode_metrics.values_history[-2]
                else:
                    previous_value = current_portfolio_value

                step_return = (current_portfolio_value - previous_value) / (previous_value + 1e-8)
                episode_step_returns.append(step_return)
                last_step_return = step_return
            else:
                last_step_return = 0.0
                episode_step_returns.append(0.0)

            episode_metrics.add_value(current_portfolio_value)

            # ===== å¤šæ ·åŒ–å¥–åŠ±è®¡ç®—ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰=====
            diversity_metrics = _calculate_diversity_metrics(action, config)
            total_reward = _calculate_total_reward(
                reward, diversity_metrics, exploration_params
            )

            # å­˜å‚¨ç»éªŒ
            replay_buffer.add(state, action.cpu(), total_reward, next_state, done)

            # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
            state = next_state
            episode_metrics.add_reward(total_reward)

            # ç»éªŒå›æ”¾å’Œç½‘ç»œæ›´æ–°
            if len(replay_buffer) >= config.batch_size:
                _update_networks(
                    actor, critic, actor_target, critic_target,
                    actor_optimizer, critic_optimizer,
                    replay_buffer, device, config
                )

            if done:
                break

        # ===== æ–°å¢ï¼šå›åˆç»“æŸåçš„å¢å¼ºå¤„ç† =====
        episode_total_return = sum(episode_step_returns)

        # æ›´æ–°è®­ç»ƒç®¡ç†å™¨
        training_manager.step(episode, episode_total_return)

        # æ›´æ–°æ€§èƒ½ç›‘æ§çª—å£
        recent_performance_window.append(episode_total_return)
        if len(recent_performance_window) > 50:
            recent_performance_window.pop(0)

        # å‚æ•°è¡°å‡æ›´æ–°
        current_exploration_rate = max(
            exploration_params['end_rate'],
            current_exploration_rate * exploration_params['decay_rate']
        )

        # è®°å½•æœ¬å›åˆæ•°æ®
        training_metrics.add_episode(episode_metrics)

        # ===== å¢å¼ºç‰ˆå›åˆæ€»ç»“ =====
        _print_enhanced_episode_summary(
            episode, episode_metrics, current_exploration_rate,
            exploration_strategy, config, actor, forced_diversification_count
        )

        # ===== æ–°å¢ï¼šæ™ºèƒ½ç­–ç•¥è°ƒæ•´ =====
        if (episode + 1) % 50 == 0:
            _perform_intelligent_adjustment(
                actor, recent_performance_window, exploration_params, episode + 1
            )

        # å¯è§†åŒ–ï¼ˆå¢å¼ºç‰ˆï¼‰
        if (episode + 1) % visualization_freq == 0:
            _visualize_enhanced_episode(
                visualizer, episode_metrics, episode + 1, save_dir, actor
            )

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if episode_metrics.total_reward > best_reward:
            best_reward = episode_metrics.total_reward
            _save_enhanced_models(actor, critic, best_reward, save_dir, episode + 1)

        # ===== æ–°å¢ï¼šå®šæœŸä¿å­˜è®­ç»ƒçŠ¶æ€ =====
        if (episode + 1) % 100 == 0:
            _save_training_checkpoint(
                actor, critic, actor_optimizer, critic_optimizer,
                episode, training_metrics, save_dir
            )

        # ===== æ–°å¢ï¼šè‡ªåŠ¨ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆè¾ƒé«˜é¢‘ç‡ï¼‰=====
        if (episode + 1) % 20 == 0:  # æ¯20å›åˆä¿å­˜ä¸€æ¬¡æœ€æ–°çŠ¶æ€
            _save_latest_checkpoint(
                actor, critic, actor_optimizer, critic_optimizer,
                episode, training_metrics, save_dir
            )

        print("-" * 80)

    # ===== å¢å¼ºç‰ˆè®­ç»ƒç»“æŸåˆ†æ =====
    _post_training_enhanced_analysis(
        training_metrics, exploration_params, config, save_dir,
        actor, forced_diversification_count
    )

    return actor, critic



# [ç§»åŠ¨æ‰€æœ‰è®­ç»ƒç›¸å…³çš„è¾…åŠ©å‡½æ•°]
def load_checkpoint(checkpoint_path, actor, critic, actor_opt, critic_opt):
    """
    åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹

    Returns:
        start_episode: ç»§ç»­è®­ç»ƒçš„èµ·å§‹å›åˆ
        training_metrics: å†å²è®­ç»ƒæŒ‡æ ‡
    """
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        # åŠ è½½ç½‘ç»œçŠ¶æ€
        actor.load_state_dict(checkpoint['actor_state_dict'])
        critic.load_state_dict(checkpoint['critic_state_dict'])
        actor_opt.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        critic_opt.load_state_dict(checkpoint['critic_optimizer_state_dict'])

        # æ¢å¤Actorå¢å¼ºçŠ¶æ€
        if 'actor_enhanced_state' in checkpoint:
            enhanced_state = checkpoint['actor_enhanced_state']
            actor.performance_memory.copy_(enhanced_state['performance_memory'])
            actor.exploration_bonus.copy_(enhanced_state['exploration_bonus'])
            actor.asset_usage_count.copy_(enhanced_state['asset_usage_count'])
            actor.training_step = enhanced_state['training_step']

            print(f"ğŸ§  å·²æ¢å¤Actorå¢å¼ºçŠ¶æ€:")
            print(f"   - è®­ç»ƒæ­¥æ•°: {actor.training_step}")
            print(f"   - æ€§èƒ½è®°å¿†èŒƒå›´: [{actor.performance_memory.min():.6f}, {actor.performance_memory.max():.6f}]")
            print(f"   - å¹³å‡èµ„äº§ä½¿ç”¨ç‡: {actor.asset_usage_count.mean():.6f}")

        # æ¢å¤è®­ç»ƒæŒ‡æ ‡
        training_metrics = checkpoint.get('training_metrics', None)
        start_episode = checkpoint['episode']

        return start_episode, training_metrics

    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        raise e



def _save_latest_checkpoint(actor, critic, actor_opt, critic_opt, episode, metrics, save_dir):
    """ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆæ¯20å›åˆï¼Œç”¨äºæ„å¤–ä¸­æ–­æ¢å¤ï¼‰"""
    checkpoint = {
        'episode': episode,
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'actor_optimizer_state_dict': actor_opt.state_dict(),
        'critic_optimizer_state_dict': critic_opt.state_dict(),
        'training_metrics': metrics,
        # Actorå¢å¼ºçŠ¶æ€
        'actor_enhanced_state': {
            'performance_memory': actor.performance_memory.clone(),
            'exploration_bonus': actor.exploration_bonus.clone(),
            'asset_usage_count': actor.asset_usage_count.clone(),
            'training_step': actor.training_step
        },
        'checkpoint_info': {
            'save_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'checkpoint_type': 'latest'
        }
    }

    # ä¿å­˜ä¸ºlatestï¼Œæ–¹ä¾¿è‡ªåŠ¨æ¢å¤
    latest_path = f"{save_dir}/latest_checkpoint.pth"
    torch.save(checkpoint, latest_path)

    # åŒæ—¶ä¿å­˜ä¸€ä¸ªå¸¦å›åˆæ•°çš„å‰¯æœ¬
    backup_path = f"{save_dir}/checkpoint_latest_episode_{episode + 1}.pth"
    torch.save(checkpoint, backup_path)

    if (episode + 1) % 100 == 0:  # åªåœ¨é‡Œç¨‹ç¢‘æ—¶æ‰“å°
        print(f"ğŸ’¾ æ›´æ–°æœ€æ–°æ£€æŸ¥ç‚¹: Episode {episode + 1}")


def auto_find_latest_checkpoint(save_dir):
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    latest_path = f"{save_dir}/latest_checkpoint.pth"

    if os.path.exists(latest_path):
        return latest_path

    # å¦‚æœlatestä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„ç¼–å·æ£€æŸ¥ç‚¹
    import glob
    checkpoint_files = glob.glob(f"{save_dir}/checkpoint_episode_*.pth")

    if checkpoint_files:
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_file = max(checkpoint_files, key=os.path.getmtime)
        return latest_file

    return None


def _print_enhanced_episode_summary(episode, episode_metrics, exploration_rate,
                                    strategy, config, actor, forced_div_count):

    final_weights = episode_metrics.weights_history[-1] if episode_metrics.weights_history else None

    if final_weights is not None:
        hhi = np.sum(final_weights ** 2)
        max_weight = np.max(final_weights)
        effective_assets = np.sum(final_weights > 0.05)
        entropy = -np.sum(final_weights * np.log(final_weights + 1e-8))
        max_entropy = np.log(len(final_weights))

        print(f"å›åˆ {episode + 1:4d} | å¥–åŠ±: {episode_metrics.total_reward:8.4f} | "
              f"ç»„åˆä»·å€¼: {episode_metrics.final_value:10.2f}")
        print(f"         | HHI: {hhi:.4f} | æœ€å¤§æƒé‡: {max_weight:.4f} | "
              f"æœ‰æ•ˆèµ„äº§: {effective_assets:2d}/{config.n_assets}")
        print(f"         | ç†µ: {entropy / max_entropy:.4f} | æ¢ç´¢ç‡: {exploration_rate:.4f} | "
              f"ç­–ç•¥: {strategy}")
        print(f"         | å¼ºåˆ¶å¤šæ ·åŒ–æ¬¡æ•°: {forced_div_count} | "
              f"æ¸©åº¦: {actor.base_temperature.item():.4f}")

        # é£é™©æç¤º
        if hhi > 0.3:
            print(f"         | âš ï¸  ä¸¥é‡é›†ä¸­é£é™©ï¼")
        elif effective_assets < config.n_assets * 0.3:
            print(f"         | âš ï¸  å¤šæ ·åŒ–ä¸è¶³")
        else:
            print(f"         | âœ… é£é™©åˆ†æ•£è‰¯å¥½")


def _perform_intelligent_adjustment(actor, performance_window, exploration_params, episode):
    """æ™ºèƒ½ç­–ç•¥è°ƒæ•´"""
    if len(performance_window) < 30:
        return

    recent_performance = np.array(performance_window[-30:])
    performance_std = np.std(recent_performance)
    performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]

    print(f"\n=== Episode {episode} æ™ºèƒ½è°ƒæ•´åˆ†æ ===")
    print(f"æœ€è¿‘30å›åˆè¡¨ç°æ ‡å‡†å·®: {performance_std:.6f}")
    print(f"è¡¨ç°è¶‹åŠ¿æ–œç‡: {performance_trend:.6f}")

    # å¦‚æœè¡¨ç°åœæ»ä¸”è¶‹åŠ¿å‘ä¸‹
    if performance_std < exploration_params['performance_stagnation_threshold'] and performance_trend < 0:
        print("ğŸ”„ æ£€æµ‹åˆ°æ€§èƒ½åœæ»ï¼Œå¯åŠ¨å¢å¼ºæ¢ç´¢æ¨¡å¼")
        actor.set_exploration_mode(high_exploration=True)

        # é¢å¤–çš„è½¯é‡ç½®
        if performance_trend < -0.001:
            print("ğŸ”„ è¶‹åŠ¿æ˜æ˜¾ä¸‹é™ï¼Œæ‰§è¡Œè½¯é‡ç½®")
            actor.force_diversify()

    # å¦‚æœè¡¨ç°ç¨³å®šå‘å¥½
    elif performance_std > 0.01 and performance_trend > 0:
        print("ğŸ“ˆ è¡¨ç°è‰¯å¥½ï¼Œåˆ‡æ¢åˆ°ç²¾ç»†è°ƒä¼˜æ¨¡å¼")
        actor.set_exploration_mode(high_exploration=False)

    print("=" * 40)


def _visualize_enhanced_episode(visualizer, episode_metrics, episode, save_dir, actor):
    """å¢å¼ºç‰ˆå¯è§†åŒ–"""
    # åŸæœ‰å¯è§†åŒ–
    _visualize_episode(visualizer, episode_metrics, episode, save_dir)

    # æ–°å¢ï¼šActorå†…éƒ¨çŠ¶æ€å¯è§†åŒ–
    if hasattr(actor, 'asset_usage_count'):
        plt.figure(figsize=(12, 8))

        # å­å›¾1ï¼šèµ„äº§ä½¿ç”¨é¢‘ç‡
        plt.subplot(2, 2, 1)
        usage_counts = actor.asset_usage_count.detach().cpu().numpy()
        plt.bar(range(len(usage_counts)), usage_counts)
        plt.title('èµ„äº§ä½¿ç”¨é¢‘ç‡')
        plt.xlabel('èµ„äº§ID')
        plt.ylabel('ä½¿ç”¨é¢‘ç‡')

        # å­å›¾2ï¼šæ€§èƒ½è®°å¿†
        plt.subplot(2, 2, 2)
        perf_memory = actor.performance_memory.detach().cpu().numpy()
        plt.bar(range(len(perf_memory)), perf_memory)
        plt.title('èµ„äº§æ€§èƒ½è®°å¿†')
        plt.xlabel('èµ„äº§ID')
        plt.ylabel('æ€§èƒ½è®°å¿†å€¼')

        # å­å›¾3ï¼šæ¢ç´¢å¥–åŠ±
        plt.subplot(2, 2, 3)
        exploration_bonus = actor.exploration_bonus.detach().cpu().numpy()
        plt.bar(range(len(exploration_bonus)), exploration_bonus)
        plt.title('æ¢ç´¢å¥–åŠ±')
        plt.xlabel('èµ„äº§ID')
        plt.ylabel('å¥–åŠ±å€¼')

        # å­å›¾4ï¼šæœ€ç»ˆæƒé‡åˆ†å¸ƒ
        plt.subplot(2, 2, 4)
        if episode_metrics.weights_history:
            final_weights = episode_metrics.weights_history[-1]
            plt.bar(range(len(final_weights)), final_weights)
            plt.title(f'Episode {episode} æœ€ç»ˆæƒé‡åˆ†å¸ƒ')
            plt.xlabel('èµ„äº§ID')
            plt.ylabel('æƒé‡')
            plt.axhline(y=1 / len(final_weights), color='r', linestyle='--', alpha=0.5, label='å‡åŒ€åˆ†å¸ƒ')
            plt.legend()

        plt.tight_layout()
        plt.savefig(f"{save_dir}/diagnostics/actor_states_episode_{episode}.png", dpi=300, bbox_inches='tight')
        plt.close()


def _save_enhanced_models(actor, critic, best_reward, save_dir, episode):
    """å¢å¼ºç‰ˆæ¨¡å‹ä¿å­˜"""
    # åŸæœ‰ä¿å­˜é€»è¾‘
    _save_best_models(actor, critic, best_reward, save_dir)

    # æ–°å¢ï¼šä¿å­˜Actorçš„å†…éƒ¨çŠ¶æ€
    actor_state = {
        'model_state_dict': actor.state_dict(),
        'performance_memory': actor.performance_memory.clone(),
        'exploration_bonus': actor.exploration_bonus.clone(),
        'asset_usage_count': actor.asset_usage_count.clone(),
        'base_temperature': actor.base_temperature.clone(),
        'training_step': actor.training_step,
        'episode': episode,
        'best_reward': best_reward
    }

    torch.save(actor_state, f"{save_dir}/best_actor_enhanced.pth")
    print(f"ğŸ’¾ å·²ä¿å­˜å¢å¼ºActorçŠ¶æ€ (Episode {episode}, Reward: {best_reward:.4f})")


def _save_training_checkpoint(actor, critic, actor_opt, critic_opt, episode, metrics, save_dir):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint = {
        'episode': episode,
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'actor_optimizer_state_dict': actor_opt.state_dict(),
        'critic_optimizer_state_dict': critic_opt.state_dict(),
        'training_metrics': metrics,
        # Actorå¢å¼ºçŠ¶æ€
        'actor_enhanced_state': {
            'performance_memory': actor.performance_memory.clone(),
            'exploration_bonus': actor.exploration_bonus.clone(),
            'asset_usage_count': actor.asset_usage_count.clone(),
            'training_step': actor.training_step
        }
    }

    torch.save(checkpoint, f"{save_dir}/checkpoint_episode_{episode}.pth")
    print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹: Episode {episode}")


def _post_training_enhanced_analysis(training_metrics, exploration_params, config,
                                     save_dir, actor, forced_div_count):
    """å¢å¼ºç‰ˆè®­ç»ƒååˆ†æ"""
    # åŸæœ‰åˆ†æ
    _post_training_analysis(training_metrics, exploration_params, config, save_dir)

    # æ–°å¢ï¼šActoræ€§èƒ½åˆ†ææŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ” å¢å¼ºActoræ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"  - æ€»è®­ç»ƒæ­¥æ•°: {actor.training_step}")
    print(f"  - å¼ºåˆ¶å¤šæ ·åŒ–æ¬¡æ•°: {forced_div_count}")
    print(f"  - æœ€ç»ˆæ¸©åº¦å‚æ•°: {actor.base_temperature.item():.4f}")

    # èµ„äº§ä½¿ç”¨åˆ†æ
    usage_counts = actor.asset_usage_count.detach().cpu().numpy()
    print(f"\nğŸ“ˆ èµ„äº§ä½¿ç”¨åˆ†æ:")
    print(f"  - æœ€å¸¸ç”¨èµ„äº§ä½¿ç”¨ç‡: {usage_counts.max():.4f}")
    print(f"  - æœ€å°‘ç”¨èµ„äº§ä½¿ç”¨ç‡: {usage_counts.min():.4f}")
    print(f"  - ä½¿ç”¨ç‡æ ‡å‡†å·®: {usage_counts.std():.4f}")
    print(f"  - æœªå……åˆ†ä½¿ç”¨èµ„äº§æ•° (<0.01): {(usage_counts < 0.01).sum()}")

    # æ€§èƒ½è®°å¿†åˆ†æ
    perf_memory = actor.performance_memory.detach().cpu().numpy()
    print(f"\nğŸ§  æ€§èƒ½è®°å¿†åˆ†æ:")
    print(f"  - æœ€ä¼˜è¡¨ç°èµ„äº§è®°å¿†å€¼: {perf_memory.max():.6f}")
    print(f"  - æœ€å·®è¡¨ç°èµ„äº§è®°å¿†å€¼: {perf_memory.min():.6f}")
    print(f"  - è®°å¿†å€¼åˆ†å¸ƒèŒƒå›´: {perf_memory.max() - perf_memory.min():.6f}")

    # ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š
    analysis_report = {
        'training_steps': actor.training_step,
        'forced_diversifications': forced_div_count,
        'final_temperature': actor.base_temperature.item(),
        'asset_usage_stats': {
            'usage_counts': usage_counts.tolist(),
            'max_usage': float(usage_counts.max()),
            'min_usage': float(usage_counts.min()),
            'std_usage': float(usage_counts.std()),
            'underused_assets': int((usage_counts < 0.01).sum())
        },
        'performance_memory_stats': {
            'memory_values': perf_memory.tolist(),
            'max_memory': float(perf_memory.max()),
            'min_memory': float(perf_memory.min()),
            'memory_range': float(perf_memory.max() - perf_memory.min())
        }
    }

    import json
    with open(f"{save_dir}/enhanced_analysis_report.json", 'w') as f:
        json.dump(analysis_report, f, indent=2)

    print(f"\nğŸ’¾ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}/enhanced_analysis_report.json")
    print("=" * 60)



# =======================================è¾…åŠ©å‡½æ•°=========================================

def _validate_environment_compatibility(env, config):
    """éªŒè¯ç¯å¢ƒä¸é…ç½®çš„å…¼å®¹æ€§"""
    if hasattr(env, 'n_assets') and env.n_assets != config.n_assets:
        raise ValueError(f"ç¯å¢ƒèµ„äº§æ•°é‡({env.n_assets})ä¸é…ç½®ä¸åŒ¹é…({config.n_assets})")

    if hasattr(env, 'feature_dim') and env.feature_dim != config.n_factors:
        print(f"è­¦å‘Š: ç¯å¢ƒç‰¹å¾ç»´åº¦({env.feature_dim})ä¸é…ç½®å› å­æ•°é‡({config.n_factors})ä¸åŒ¹é…")

def _validate_state_dimensions(state, config):
    """éªŒè¯çŠ¶æ€ç»´åº¦"""
    expected_shape = (config.n_assets, config.lookback_window, config.n_factors)
    if hasattr(state, 'shape'):
        if state.shape != expected_shape:
            print(f"è­¦å‘Š: çŠ¶æ€ç»´åº¦{state.shape}ä¸æœŸæœ›ç»´åº¦{expected_shape}ä¸åŒ¹é…")


def _calculate_diversity_metrics(action, config):
    """è®¡ç®—å¤šæ ·åŒ–æŒ‡æ ‡"""
    weights_np = action.detach().cpu().numpy()

    # HerfindahlæŒ‡æ•°
    herfindahl_index = np.sum(weights_np ** 2)
    diversity_score_1 = 1.0 - herfindahl_index

    # æœ‰æ•ˆèµ„äº§æ•°é‡
    effective_assets = 1.0 / herfindahl_index
    normalized_effective_assets = effective_assets / config.n_assets

    # ç»¼åˆå¤šæ ·åŒ–åˆ†æ•°
    diversity_score = 0.7 * diversity_score_1 + 0.3 * normalized_effective_assets

    return {
        'herfindahl_index': herfindahl_index,
        'diversity_score': diversity_score,
        'effective_assets': effective_assets,
        'max_weight': np.max(weights_np)
    }

def _calculate_total_reward(base_reward, diversity_metrics, exploration_params):
    """è®¡ç®—æ€»å¥–åŠ±"""
    # å¤šæ ·åŒ–å¥–åŠ±
    diversity_reward = exploration_params['diversity_reward_weight'] * diversity_metrics['diversity_score']

    # é›†ä¸­åº¦æƒ©ç½š
    concentration_penalty = 0
    max_weight = diversity_metrics['max_weight']
    if max_weight > exploration_params['concentration_threshold']:
        concentration_penalty = -0.1 * (max_weight - exploration_params['concentration_threshold']) / \
                               (1 - exploration_params['concentration_threshold'])

    return base_reward + diversity_reward + concentration_penalty

def _update_networks(actor, critic, actor_target, critic_target,
                    actor_optimizer, critic_optimizer, replay_buffer, device, config):
    """æ›´æ–°ç½‘ç»œå‚æ•°"""
    states, actions, rewards, next_states, dones = replay_buffer.sample(config.batch_size)

    states = states.to(device)
    actions = actions.to(device)
    rewards = rewards.to(device)
    next_states = next_states.to(device)
    dones = dones.to(device)

    # Criticæ›´æ–°
    with torch.no_grad():
        next_actions = actor_target(next_states)
        if next_actions.dim() > 2:
            next_actions = next_actions.squeeze(1)
        next_q_values = critic_target(next_states, next_actions)
        target_q_values = rewards + (1 - dones) * config.gamma * next_q_values

    current_q_values = critic(states, actions)
    critic_loss = F.mse_loss(current_q_values, target_q_values.detach())

    critic_optimizer.zero_grad()
    critic_loss.backward()
    torch.nn.utils.clip_grad_norm_(critic.parameters(), config.grad_clip_norm)
    critic_optimizer.step()

    # Actoræ›´æ–°
    actor_loss = -critic(states, actor(states)).mean()

    actor_optimizer.zero_grad()
    actor_loss.backward()
    torch.nn.utils.clip_grad_norm_(actor.parameters(), config.grad_clip_norm)
    actor_optimizer.step()

    # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
    for target_param, param in zip(actor_target.parameters(), actor.parameters()):
        target_param.data.copy_(config.tau * param.data + (1 - config.tau) * target_param.data)

    for target_param, param in zip(critic_target.parameters(), critic.parameters()):
        target_param.data.copy_(config.tau * param.data + (1 - config.tau) * target_param.data)


def _visualize_episode(visualizer, episode_metrics, episode_num, save_dir):
    """ç”Ÿæˆå›åˆå¯è§†åŒ–"""
    visualizer.visualize_episode_allocation(
        episode_metrics.weights_history,
        episode_metrics.values_history,
        episode_num,
        save_path=f"{save_dir}/visualizations"
    )

def _save_best_models(actor, critic, best_reward, save_dir):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    torch.save(actor.state_dict(), f'{save_dir}/best_actor_improved.pth')
    torch.save(critic.state_dict(), f'{save_dir}/best_critic_improved.pth')
    print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå¥–åŠ±: {best_reward:.4f}")

def _post_training_analysis(training_metrics, exploration_params, config, save_dir):
    """è®­ç»ƒååˆ†æå’Œå¯è§†åŒ–"""
    print("\n=== è®­ç»ƒå®Œæˆåˆ†æ ===")
    final_stats = training_metrics.get_final_stats()

    print(f"æœ€ç»ˆæ¢ç´¢ç‡: {exploration_params['end_rate']:.3f}")
    print(f"å¹³å‡å¤šæ ·åŒ–ç¨‹åº¦: {final_stats['avg_diversity']:.3f}")
    print(f"å¤šæ ·åŒ–ç¨‹åº¦æ ‡å‡†å·®: {final_stats['diversity_std']:.3f}")
    print(f"å®é™…å¹³å‡æœ‰æ•ˆèµ„äº§æ•°: {final_stats['avg_effective_assets']:.1f}")

    # ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–
    _generate_training_visualizations(training_metrics, exploration_params, config, save_dir)

def _generate_training_visualizations(training_metrics, exploration_params, config, save_dir):
    """ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨"""
    plt.figure(figsize=(20, 12))

    # åˆ›å»º6ä¸ªå­å›¾
    plots_config = [
        {'data': training_metrics.diversity_history, 'title': 'æŠ•èµ„ç»„åˆå¤šæ ·åŒ–ç¨‹åº¦å˜åŒ–',
         'ylabel': 'å¤šæ ·åŒ–åˆ†æ•° (0-1)', 'subplot': (2, 3, 1)},

        {'data': training_metrics.effective_assets_history, 'title': 'æœ‰æ•ˆèµ„äº§æ•°é‡å˜åŒ–',
         'ylabel': 'æœ‰æ•ˆèµ„äº§æ•°', 'subplot': (2, 3, 2),
         'hline': {'y': config.n_assets, 'label': 'ç†è®ºæœ€å¤§å€¼'}},

        {'data': training_metrics.exploration_rates, 'title': 'æ¢ç´¢ç‡å˜åŒ–',
         'ylabel': 'æ¢ç´¢ç‡', 'subplot': (2, 3, 3)},

        {'data': training_metrics.get_final_weights(), 'title': 'æœ€ç»ˆæƒé‡åˆ†å¸ƒ',
         'ylabel': 'æƒé‡', 'subplot': (2, 3, 4), 'plot_type': 'bar'},

        {'data': training_metrics.max_weights_history, 'title': 'æœ€å¤§å•èµ„äº§æƒé‡å˜åŒ–',
         'ylabel': 'æœ€å¤§æƒé‡', 'subplot': (2, 3, 5),
         'hline': {'y': config.max_weight, 'label': 'æœ€å¤§æƒé‡é™åˆ¶'}},

        {'data': training_metrics.min_weights_history, 'title': 'æœ€å°å•èµ„äº§æƒé‡å˜åŒ–',
         'ylabel': 'æœ€å°æƒé‡', 'subplot': (2, 3, 6),
         'hline': {'y': config.min_weight, 'label': 'æœ€å°æƒé‡é™åˆ¶'}}
    ]

    for plot_config in plots_config:
        plt.subplot(*plot_config['subplot'])

        if plot_config.get('plot_type') == 'bar':
            plt.bar(range(len(plot_config['data'])), plot_config['data'])
            plt.xlabel('èµ„äº§ç¼–å·')
        else:
            plt.plot(plot_config['data'])
            plt.xlabel('Episode')

        plt.title(plot_config['title'])
        plt.ylabel(plot_config['ylabel'])
        plt.grid(True)

        if 'hline' in plot_config:
            hline = plot_config['hline']
            plt.axhline(y=hline['y'], color='r', linestyle='--', label=hline['label'])
            plt.legend()

    plt.tight_layout()
    plt.savefig(f'{save_dir}/exploration_analysis_improved.png', dpi=300, bbox_inches='tight')
    plt.show()

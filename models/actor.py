# models/actor.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from config import config
from .components import NonStationaryTransformer, AssetGCN

class Actor(nn.Module):
    def __init__(self, env, num_heads=4, min_weight=0.01, max_weight=0.15, temperature_start=1.0):
        super().__init__()
        self.env = env
        self.n_assets = env.n_assets
        self.window_size = env.window_size
        self.feature_dim = env.feature_dim
        self.min_weight = min_weight
        self.max_weight = max_weight

        # ç¡®ä¿num_headsèƒ½è¢«feature_dimæ•´é™¤
        if self.feature_dim % num_heads != 0:
            for h in [8, 6, 4, 3, 2, 1]:
                if self.feature_dim % h == 0:
                    num_heads = h
                    break
            print(f"è­¦å‘Šï¼šActorè°ƒæ•´num_headsä¸º{num_heads}ä»¥é€‚åº”feature_dim={self.feature_dim}")

        # æ—¶é—´åºåˆ—æ¨¡å—
        self.nst_modules = nn.ModuleList([
            NonStationaryTransformer(self.feature_dim, num_heads=num_heads)
            for _ in range(self.n_assets)
        ])

        # èµ„äº§ä¾èµ–æ¨¡å—
        self.gcn = AssetGCN(self.feature_dim, hidden_dim=64, window_size=self.window_size, n_assets=self.n_assets)

        # ç‰¹å¾èåˆå±‚
        fusion_input_dim = self.feature_dim + 64
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )

        # èµ„äº§æ³¨æ„åŠ›å±‚
        self.asset_attention = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

        # å†³ç­–ç½‘ç»œ - å¢åŠ å¤šå¤´è¾“å‡º
        self.decision_net = nn.Sequential(
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
        )

        # å¤šå¤´è¾“å‡º - å…³é”®æ”¹è¿›ï¼
        self.policy_head = nn.Linear(128, self.n_assets)  # ä¸»ç­–ç•¥å¤´
        self.exploration_head = nn.Linear(128, self.n_assets)  # æ¢ç´¢ç­–ç•¥å¤´

        # ç­–ç•¥æ··åˆæƒé‡
        self.policy_mixer = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )


        # å±‚æ ‡å‡†åŒ–
        self.layer_norm = nn.LayerNorm(128)

        # åŠ¨æ€æƒé‡åç½®ç³»ç»Ÿ - å…³é”®æ”¹è¿›ï¼
        self.register_buffer('performance_memory', torch.zeros(self.n_assets))
        self.register_buffer('exploration_bonus', torch.ones(self.n_assets))
        self.memory_decay = 0.95
        self.exploration_decay = 0.99

        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•° - æ”¹ä¸ºè‡ªé€‚åº”
        self.base_temperature = nn.Parameter(torch.tensor(temperature_start))
        self.adaptive_temp_scale = nn.Parameter(torch.tensor(0.1))

        # é›†ä¸­åº¦ç›‘æ§å’Œæƒ©ç½š
        self.concentration_threshold = 0.3  # HHIé˜ˆå€¼
        self.concentration_penalty = 0.1

        # å¤šæ ·æ€§å¥–åŠ±æœºåˆ¶
        self.diversity_bonus_scale = 0.05
        self.register_buffer('asset_usage_count', torch.zeros(self.n_assets))

        # å‘¨æœŸæ€§é‡ç½®æœºåˆ¶
        self.reset_period = 500  # æ¯500æ­¥æ£€æŸ¥ä¸€æ¬¡
        self.performance_window = 50  # æ€§èƒ½ç›‘æ§çª—å£
        self.register_buffer('recent_performance', torch.zeros(self.performance_window))
        self.performance_idx = 0

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

        # ç›‘æ§ç»Ÿè®¡
        self.training_step = 0
        self.last_hhi = 0.0

    def _initialize_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.8)  # ç¨å¾®å°ä¸€ç‚¹çš„gain
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        # ç‰¹åˆ«åˆå§‹åŒ–è¾“å‡ºå±‚
        with torch.no_grad():
            # ä¸»ç­–ç•¥å¤´ï¼šåå‘å‡åŒ€åˆ†å¸ƒ
            nn.init.xavier_uniform_(self.policy_head.weight, gain=0.1)
            nn.init.constant_(self.policy_head.bias, 0)

            # æ¢ç´¢ç­–ç•¥å¤´ï¼šæ›´å¤§çš„éšæœºæ€§
            nn.init.xavier_uniform_(self.exploration_head.weight, gain=0.3)
            nn.init.constant_(self.exploration_head.bias, 0)

    def forward(self, state, add_noise=True, training_mode=None, portfolio_return=None):
        """
        å¢å¼ºç‰ˆå‰å‘ä¼ æ’­
        portfolio_return: ç”¨äºæ›´æ–°æ€§èƒ½è®°å¿†
        """
        if training_mode is not None:
            self.training = training_mode

        batch_size, n_assets, window_size, feature_dim = state.shape
        self.training_step += 1

        # æ›´æ–°æ€§èƒ½è®°å¿†
        if portfolio_return is not None and self.training:
            self._update_performance_memory(portfolio_return)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®
        if self.training_step % self.reset_period == 0:
            self._check_and_reset()

        # 1. ç‰¹å¾æå–ï¼ˆä¸åŸå§‹ä»£ç ç›¸åŒï¼‰
        temporal_features = []
        for i in range(self.n_assets):
            asset_data = state[:, i, :, :]
            asset_data = asset_data.permute(1, 0, 2)
            asset_feat = self.nst_modules[i](asset_data)
            asset_feat = asset_feat.permute(1, 0, 2)
            temporal_features.append(asset_feat.mean(dim=1))

        temporal_features = torch.stack(temporal_features, dim=1)

        # 2. èµ„äº§ä¾èµ–ç‰¹å¾æå–
        x_reshaped = state.view(-1, window_size, feature_dim)
        dependency_features = self.gcn(x_reshaped)
        dependency_features = dependency_features.view(batch_size, n_assets, -1)

        # 3. ç‰¹å¾èåˆ
        fused = torch.cat([temporal_features, dependency_features], dim=2)
        asset_attention = self.asset_attention(fused)
        asset_attention = torch.softmax(asset_attention, dim=1)
        global_features = torch.sum(fused * asset_attention, dim=1)

        fused = self.fusion(global_features)
        fused = self.layer_norm(fused)

        # 4. å¤šå¤´å†³ç­– - å…³é”®æ”¹è¿›ï¼
        shared_features = self.decision_net(fused)

        # ä¸»ç­–ç•¥logits
        main_logits = self.policy_head(shared_features)

        # æ¢ç´¢ç­–ç•¥logitsï¼ˆæ›´éšæœºï¼‰
        explore_logits = self.exploration_head(shared_features)

        # åŠ¨æ€æ··åˆæƒé‡
        mix_weight = self.policy_mixer(shared_features)

        # æ ¹æ®é›†ä¸­åº¦åŠ¨æ€è°ƒæ•´æ··åˆæ¯”ä¾‹
        current_concentration = self._estimate_concentration(main_logits)
        if current_concentration > self.concentration_threshold:
            # å¦‚æœè¿‡äºé›†ä¸­ï¼Œå¢åŠ æ¢ç´¢æƒé‡
            mix_weight = mix_weight * 0.3  # é™ä½ä¸»ç­–ç•¥æƒé‡

        # æ··åˆä¸¤ä¸ªç­–ç•¥
        combined_logits = mix_weight * main_logits + (1 - mix_weight) * explore_logits

        # 5. æ·»åŠ åŠ¨æ€åç½® - å…³é”®æ”¹è¿›ï¼
        dynamic_bias = self._compute_dynamic_bias()
        final_logits = combined_logits + dynamic_bias.unsqueeze(0)

        # 6. åº”ç”¨çº¦æŸè·å¾—æƒé‡
        weights = self._enhanced_constrained_softmax(final_logits, add_noise)

        # 7. æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
        if self.training:
            self._update_usage_stats(weights)

        # 8. ç›‘æ§å’Œè¯Šæ–­
        if self.training_step % 100 == 0:
            self._enhanced_diagnose(weights, final_logits, self.training_step)

        return weights


    def _compute_dynamic_bias(self):
        """è®¡ç®—åŠ¨æ€åç½® - æ ¸å¿ƒåè¿‡æ—©æ”¶æ•›æœºåˆ¶"""
        # 1. æ€§èƒ½è®°å¿†åç½®ï¼šè¡¨ç°å·®çš„èµ„äº§è·å¾—æ¢ç´¢å¥–åŠ±
        performance_bias = -self.performance_memory * 0.1

        # 2. æ¢ç´¢å¥–åŠ±ï¼šä½¿ç”¨å°‘çš„èµ„äº§è·å¾—å¥–åŠ±
        exploration_bias = torch.log(self.exploration_bonus + 1e-8) * self.diversity_bonus_scale

        # 3. åé›†ä¸­åç½®ï¼šæƒé‡è¿‡é«˜çš„èµ„äº§è¢«æƒ©ç½š
        usage_penalty = -torch.log(self.asset_usage_count + 1.0) * 0.02

        # 4. éšæœºæ¢ç´¢åç½®
        random_bias = torch.randn(self.n_assets, device=self.performance_memory.device) * 0.01

        total_bias = performance_bias + exploration_bias + usage_penalty + random_bias
        return total_bias

    def _enhanced_constrained_softmax(self, logits, add_noise=True):
        """å¢å¼ºç‰ˆçº¦æŸsoftmax"""
        # 1. è‡ªé€‚åº”æ¸©åº¦
        concentration = self._estimate_concentration(logits)
        if concentration > self.concentration_threshold:
            # å¦‚æœè¿‡äºé›†ä¸­ï¼Œæé«˜æ¸©åº¦å¢åŠ éšæœºæ€§
            temperature = self.base_temperature + self.adaptive_temp_scale * (
                        concentration - self.concentration_threshold) * 10
        else:
            temperature = self.base_temperature

        temperature = torch.clamp(temperature, min=0.3, max=3.0)
        scaled_logits = logits / temperature

        # 2. æ•°å€¼ç¨³å®šçš„softmax
        weights = self._stable_softmax(scaled_logits)

        # 3. é›†ä¸­åº¦æƒ©ç½š
        weights = self._apply_concentration_penalty(weights)

        # 4. å¤šæ ·æ€§å¥–åŠ±
        if self.training:
            weights = self._apply_diversity_bonus(weights)

        # 5. æ·»åŠ æ¢ç´¢å™ªå£°
        if add_noise and self.training:
            weights = self._enhanced_exploration_noise(weights)

        return weights

    def _estimate_concentration(self, logits):
        """ä¼°è®¡å½“å‰é›†ä¸­åº¦"""
        with torch.no_grad():
            weights = F.softmax(logits, dim=1)
            hhi = (weights ** 2).sum(dim=1).mean()
            return hhi.item()

    def _apply_concentration_penalty(self, weights):
        """åº”ç”¨é›†ä¸­åº¦æƒ©ç½š"""
        hhi = (weights ** 2).sum(dim=1, keepdim=True)
        penalty_mask = (hhi > self.concentration_threshold).float()

        if penalty_mask.sum() > 0:
            # å¯¹è¿‡äºé›†ä¸­çš„ç»„åˆè¿›è¡Œè½¯æ€§å‡åŒ€åŒ–
            uniform_weights = torch.ones_like(weights) / self.n_assets
            penalty_strength = (hhi - self.concentration_threshold) * self.concentration_penalty
            penalty_strength = torch.clamp(penalty_strength, 0, 0.3)

            weights = weights * (1 - penalty_strength) + uniform_weights * penalty_strength
            # é‡æ–°æ ‡å‡†åŒ–
            weights = weights / weights.sum(dim=1, keepdim=True)

        return weights

    def _apply_diversity_bonus(self, weights):
        """åº”ç”¨å¤šæ ·æ€§å¥–åŠ±"""
        # ç»™ä½¿ç”¨è¾ƒå°‘çš„èµ„äº§é¢å¤–çš„æƒé‡æå‡
        underused_bonus = (1.0 / (self.asset_usage_count + 1.0)) * self.diversity_bonus_scale
        underused_bonus = underused_bonus / underused_bonus.sum()  # æ ‡å‡†åŒ–

        # è½¯æ€§æ··åˆ
        mix_ratio = 0.95
        enhanced_weights = mix_ratio * weights + (1 - mix_ratio) * underused_bonus.unsqueeze(0)

        return enhanced_weights / enhanced_weights.sum(dim=1, keepdim=True)

    def _enhanced_exploration_noise(self, weights):
        """å¢å¼ºç‰ˆæ¢ç´¢å™ªå£°"""
        if not self.training:
            return weights

        # 1. åŸºç¡€Dirichletå™ªå£°
        alpha = weights * self.n_assets * 2.0 + 0.1
        alpha = torch.clamp(alpha, min=0.1, max=10.0)

        try:
            gamma_samples = torch.distributions.Gamma(alpha, 1.0).sample()
            dirichlet_noise = gamma_samples / gamma_samples.sum(dim=1, keepdim=True)
        except:
            # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å‡åŒ€å™ªå£°
            dirichlet_noise = torch.ones_like(weights) / self.n_assets

        # 2. åé›†ä¸­å™ªå£°ï¼šå¯¹é›†ä¸­åº¦é«˜çš„ç»„åˆå¢åŠ æ›´å¤šå™ªå£°
        hhi = (weights ** 2).sum(dim=1, keepdim=True)
        noise_strength = torch.clamp((hhi - 0.1) * 0.5, 0.05, 0.3)

        # 3. æ··åˆ
        noisy_weights = (1 - noise_strength) * weights + noise_strength * dirichlet_noise

        return noisy_weights

    def _update_performance_memory(self, portfolio_return):
        """æ›´æ–°æ€§èƒ½è®°å¿†"""
        if portfolio_return is not None:
            # æ›´æ–°æœ€è¿‘æ€§èƒ½è®°å½•
            self.recent_performance[self.performance_idx] = portfolio_return
            self.performance_idx = (self.performance_idx + 1) % self.performance_window

            # ç®€å•çš„æ€§èƒ½å½’å› ï¼ˆè¿™é‡Œå¯ä»¥æ”¹è¿›ï¼‰
            # å‡è®¾è¡¨ç°å¥½æ—¶æ‰€æœ‰å½“å‰æƒé‡çš„èµ„äº§éƒ½è·å¾—æ­£é¢è®°å¿†
            current_weights = getattr(self, '_last_weights', torch.ones(self.n_assets) / self.n_assets)

            # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
            performance_signal = portfolio_return * current_weights
            self.performance_memory = (self.memory_decay * self.performance_memory +
                                       (1 - self.memory_decay) * performance_signal)

    def _update_usage_stats(self, weights):
        """æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        # æ›´æ–°èµ„äº§ä½¿ç”¨è®¡æ•°
        usage_this_step = (weights.detach().cpu() > 0.02).float().mean(dim=0)  # æƒé‡>2%ç®—ä½œä½¿ç”¨
        self.asset_usage_count = (self.asset_usage_count * 0.999 +
                                  usage_this_step.to(self.asset_usage_count.device) * 0.001)

        # æ›´æ–°æ¢ç´¢å¥–åŠ±
        self.exploration_bonus *= self.exploration_decay
        unused_assets = (usage_this_step < 0.1)
        self.exploration_bonus[unused_assets] += 0.1  # ç»™æœªä½¿ç”¨çš„èµ„äº§å¥–åŠ±

        # è®°å½•æœ€åæƒé‡ç”¨äºæ€§èƒ½å½’å› 
        self._last_weights = weights[0].detach().cpu()

    def _check_and_reset(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®"""
        if self.recent_performance.sum() != 0:  # ç¡®ä¿æœ‰æ•°æ®
            recent_std = self.recent_performance.std()
            recent_mean = self.recent_performance.mean()

            # å¦‚æœæœ€è¿‘è¡¨ç°åœæ»ä¸”é›†ä¸­åº¦è¿‡é«˜
            if recent_std < 0.001 and self.last_hhi > 0.25:
                print(f"Step {self.training_step}: æ£€æµ‹åˆ°è¿‡æ—©æ”¶æ•›ï¼Œæ‰§è¡Œè½¯é‡ç½®")
                self._soft_reset()

    def _soft_reset(self):
        """è½¯é‡ç½®ï¼šä¸å®Œå…¨é‡ç½®ï¼Œåªè°ƒæ•´å…³é”®å‚æ•°"""
        # 1. é‡ç½®æ¢ç´¢å¥–åŠ±
        self.exploration_bonus.fill_(1.0)

        # 2. é‡ç½®ä½¿ç”¨ç»Ÿè®¡
        self.asset_usage_count.fill_(0.0)

        # 3. é‡ç½®æ€§èƒ½è®°å¿†ï¼ˆä¿ç•™ä¸€äº›å†å²ï¼‰
        self.performance_memory *= 0.5

        # 4. å¢åŠ æ¸©åº¦å‚æ•°
        with torch.no_grad():
            self.base_temperature.data = torch.clamp(self.base_temperature + 0.2, 0.5, 2.0)

        # 5. ç»™æ¢ç´¢å¤´æ·»åŠ å°çš„éšæœºæ‰°åŠ¨
        with torch.no_grad():
            self.exploration_head.weight.data += torch.randn_like(self.exploration_head.weight) * 0.01

        print("è½¯é‡ç½®å®Œæˆï¼Œæ¢å¤æ¢ç´¢èƒ½åŠ›")

    def _stable_softmax(self, logits):
        """æ•°å€¼ç¨³å®šçš„Softmaxå®ç°"""
        max_logits = torch.max(logits, dim=1, keepdim=True)[0]
        exp_logits = torch.exp(logits - max_logits)
        sum_exp = torch.sum(exp_logits, dim=1, keepdim=True) + 1e-8
        weights = exp_logits / sum_exp
        return weights

    def _enhanced_diagnose(self, weights, logits, step):
        """å¢å¼ºç‰ˆè¯Šæ–­"""
        with torch.no_grad():
            weights_np = weights[0].detach().cpu().numpy()
            logits_np = logits[0].detach().cpu().numpy()

            # è®¡ç®—å…³é”®æŒ‡æ ‡
            hhi = np.sum(weights_np ** 2)
            self.last_hhi = hhi
            entropy = -np.sum(weights_np * np.log(weights_np + 1e-8))
            max_entropy = np.log(len(weights_np))

            print(f"\n=== Step {step} Enhanced Diagnosis ===")
            print(f"HHI: {hhi:.4f} (ç›®æ ‡: 0.05-0.15)")
            print(f"Entropy: {entropy:.4f} / {max_entropy:.4f} = {entropy / max_entropy:.4f}")
            print(f"Temperature: {self.base_temperature.item():.4f}")
            print(f"Top 3 weights: {sorted(weights_np, reverse=True)[:3]}")
            print(f"Assets > 5%: {(weights_np > 0.05).sum()}")

            # æ£€æŸ¥é›†ä¸­åº¦
            if hhi > 0.3:
                print("ğŸš¨ CRITICAL: ä¸¥é‡è¿‡åº¦é›†ä¸­ï¼")
            elif hhi > 0.2:
                print("âš ï¸ WARNING: è¿‡åº¦é›†ä¸­")
            elif hhi < 0.04:
                print("âš ï¸ WARNING: è¿‡åº¦åˆ†æ•£")
            else:
                print("âœ… GOOD: é›†ä¸­åº¦åˆç†")

            # æ˜¾ç¤ºæ¢ç´¢çŠ¶æ€
            unused_assets = (self.asset_usage_count < 0.01).sum()
            print(f"æœªå……åˆ†ä½¿ç”¨çš„èµ„äº§æ•°: {unused_assets}")

    def get_enhanced_stats(self, weights):
        """è·å–å¢å¼ºç»Ÿè®¡ä¿¡æ¯"""
        with torch.no_grad():
            stats = {
                'max_weight': weights.max().item(),
                'min_weight': weights.min().item(),
                'std_weight': weights.std().item(),
                'effective_assets': (weights > 0.05).sum().item(),
                'hhi': (weights ** 2).sum().item(),
                'entropy': -(weights * torch.log(weights + 1e-8)).sum().item(),
                'temperature': self.base_temperature.item(),
                'unused_assets': (self.asset_usage_count < 0.01).sum().item(),
            }
            return stats

    def force_diversify(self):
        """å¼ºåˆ¶å¤šæ ·åŒ– - å¤–éƒ¨è°ƒç”¨æ¥å£"""
        print("æ‰§è¡Œå¼ºåˆ¶å¤šæ ·åŒ–...")
        self._soft_reset()
        # é¢å¤–ï¼šä¸´æ—¶æé«˜æ¢ç´¢å™ªå£°
        self.temp_high_exploration = True

    def set_exploration_mode(self, high_exploration=False):
        """è®¾ç½®æ¢ç´¢æ¨¡å¼"""
        if high_exploration:
            with torch.no_grad():
                self.base_temperature.data = torch.tensor(2.0)
            self.diversity_bonus_scale = 0.1
        else:
            with torch.no_grad():
                self.base_temperature.data = torch.tensor(1.0)
            self.diversity_bonus_scale = 0.05


# ä½¿ç”¨ç¤ºä¾‹å’Œè®­ç»ƒå»ºè®®
class EnhancedTrainingManager:
    """å¢å¼ºç‰ˆè®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, actor):
        self.actor = actor
        self.concentration_history = []
        self.performance_history = []

    def step(self, episode, portfolio_return=None):
        """æ¯æ­¥è°ƒç”¨"""
        # è®°å½•è¡¨ç°
        if portfolio_return is not None:
            self.performance_history.append(portfolio_return)

        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if episode % 100 == 0:
            self._check_and_adjust(episode)

    def _check_and_adjust(self, episode):
        """æ£€æŸ¥å¹¶è°ƒæ•´è®­ç»ƒç­–ç•¥"""
        if len(self.performance_history) < 50:
            return

        recent_perf = self.performance_history[-50:]
        perf_std = np.std(recent_perf)

        # å¦‚æœè¡¨ç°æ³¢åŠ¨å¾ˆå°ï¼Œå¯èƒ½é™·å…¥äº†å±€éƒ¨æœ€ä¼˜
        if perf_std < 0.001:
            print(f"Episode {episode}: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ—©æ”¶æ•›ï¼Œå¢å¼ºæ¢ç´¢")
            self.actor.set_exploration_mode(high_exploration=True)
        else:
            self.actor.set_exploration_mode(high_exploration=False)




